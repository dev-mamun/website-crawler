# Website Crawler

This service crawls the FDIC website starting from the specified URL, saves HTML pages as PDFs, and downloads existing
PDFs.

## Features

- Recursive crawling of the FDIC website
- HTML pages saved as PDF with preserved formatting
- PDF downloads with standardized naming
- Scheduled crawling (default: every 12 hours)
- API endpoints for monitoring and manual triggering

## Project Structure

      website-crawler/
      â”œâ”€â”€ app/ # Application source code
      â”‚ â”œâ”€â”€ init.py # Python package marker
      â”‚ â”œâ”€â”€ main.py # FastAPI application setup
      â”‚ â”œâ”€â”€ config.py # Configuration settings
      â”‚ â”œâ”€â”€ crawler/ # Crawler module
      â”‚ â”‚ â”œâ”€â”€ init.py
      â”‚ â”‚ â”œâ”€â”€ crawler.py # Main crawling logic
      â”‚ â”‚ â”œâ”€â”€ pdf_generator.py # PDF generation with Playwright
      â”‚ â”‚ â””â”€â”€ scheduler.py # APScheduler setup
      â”‚ â”œâ”€â”€ models/ # Database models
      â”‚ â”‚ â”œâ”€â”€ init.py
      â”‚ â”‚ â””â”€â”€ crawl_job.py # Job tracking model
      â”‚ â”œâ”€â”€ storage/ # Storage handling
      â”‚ â”‚ â”œâ”€â”€ init.py
      â”‚ â”‚ â””â”€â”€ file_storage.py # File operations
      â”‚ â””â”€â”€ utils/ # Utility functions
      â”‚ â”œâ”€â”€ init.py
      â”‚ â”œâ”€â”€ logger.py # Logging configuration
      â”‚ â””â”€â”€ helpers.py # Helper functions
      â”œâ”€â”€ tests/ # Test files
      â”‚ â”œâ”€â”€ init.py
      â”‚ â””â”€â”€ test_crawler.py
      â”œâ”€â”€ storage/ # PDF storage (auto-created)
      â”‚ â”œâ”€â”€ html_pdfs/ # HTML-to-PDF conversions
      â”‚ â””â”€â”€ downloaded_pdfs/ # Direct PDF downloads
      â”œâ”€â”€ Pipfile # Pipenv dependencies
      â”œâ”€â”€ Pipfile.lock
      â”œâ”€â”€ README.md # This documentation
      â”œâ”€â”€ .env # Environment variables
      â””â”€â”€ .gitignore # Git ignore rules

### Key Directories Explained:

1. **app/** - Core application logic:
    - ğŸ`main.py`: FastAPI application entry point
    - ğŸ`config.py`: Centralized configuration
    - ğŸ“`crawler/`: Website crawling and PDF generation
    - ğŸ“`models/`: Database models (if using DB)
    - ğŸ“`storage/`: File handling operations
    - ğŸ“`utils/`: Common utilities

2. **ğŸ“ storage/** - Auto-created directories:
    - ğŸ“`html_pdfs/`: Stores converted HTML pages as PDFs
    - ğŸ“`downloaded_pdfs/`: Stores directly downloaded PDFs

3. **Root Files**:
    - `Pipfile`: Python dependency management
    - `.env`: Environment configuration
    - `.gitignore`: Version control exclusions

## Setup

1. **Prerequisites**:
    - Python 3.9+
    - Node.js (for Playwright)
    - pipenv

2. **Install dependencies**:
   ```bash
   pipenv install
   pipenv run playwright install
   pipenv run playwright install-deps

3. **Configuration**:
    - Create the `.env` file
        - SCHEDULE_INTERVAL=12 # hours between crawls
        - CRAWL_DEPTH=3
        - MAX_PAGES=50
        - HTML_PDF_DIR=storage/html_pdfs
        - DOWNLOADED_PDF_DIR=storage/downloaded_pdfs
        - BASE_URL=https://www.fdic.gov/risk-management-manual-examination-policies # URL to crawl

4. **Run the application**:
   ```bash
   pipenv shell
   pipenv run uvicorn app.main:app --reload

5. **Access the API**:
    - http://localhost:8000/docs (Swagger UI)

6. **API Endpoints**:

       GET /: Service status
       GET /pdfs: List all stored PDFs
       GET /stats: Get crawler statistics
       POST /run-crawler: Trigger crawler manually
   
